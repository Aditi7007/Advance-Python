{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgrZEmc05JmUxS+o1MCik3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"2kIjGGsV6fSg","executionInfo":{"status":"ok","timestamp":1743655060771,"user_tz":-330,"elapsed":6371,"user":{"displayName":"ADITI SINGH","userId":"15686955306504732234"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","source":[],"metadata":{"id":"XUxIMmJ58Kyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"\"\" the quick brown for jumps over the lazy dog.A quick brown fox jumps over the lazy dog. \"\"\"\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words  = len(tokenizer.word_index) + 1"],"metadata":{"id":"qQkJwXaS8zAj","executionInfo":{"status":"ok","timestamp":1743655403388,"user_tz":-330,"elapsed":18,"user":{"displayName":"ADITI SINGH","userId":"15686955306504732234"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print()"],"metadata":{"id":"If-e7C-i9heV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_sequences = []\n","for line in text.split('\\n'):\n","  if line.strip() == '':\n","    continue\n","  token_list = tokenizer.texts_to_sequences([line])[0]\n","  for i in range(1, len(token_list)):\n","    n_gram_sequence = token_list[:i+1]\n","    input_sequences.append(n_gram_sequence)"],"metadata":{"id":"k0aE5mC-9xgk","executionInfo":{"status":"ok","timestamp":1743655622059,"user_tz":-330,"elapsed":8,"user":{"displayName":"ADITI SINGH","userId":"15686955306504732234"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"],"metadata":{"id":"9_92Ugyd-YBz","executionInfo":{"status":"ok","timestamp":1743655678675,"user_tz":-330,"elapsed":14,"user":{"displayName":"ADITI SINGH","userId":"15686955306504732234"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["x = input_sequences[:, :-1]\n","y = input_sequences[:, -1]\n","y = to_categorical(y, num_classes=total_words)"],"metadata":{"id":"9t3Zji5D-jrD","executionInfo":{"status":"ok","timestamp":1743655700653,"user_tz":-330,"elapsed":11,"user":{"displayName":"ADITI SINGH","userId":"15686955306504732234"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(\"\\nSample input sequences:\")\n","for i, seq in enumerate(X[:3]):\n","    print(f\"Input: {seq} -> Target: {np.argmax(y[i])} ({tokenizer.index_word[np.argmax(y[i])]})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"JqApoyhR-s0O","executionInfo":{"status":"error","timestamp":1743656061604,"user_tz":-330,"elapsed":70,"user":{"displayName":"ADITI SINGH","userId":"15686955306504732234"}},"outputId":"8d6adc77-5b91-4d3a-9c66-6bd22e262ceb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample input sequences:\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'X' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c6914dc30106>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSample input sequences:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input: {seq} -> Target: {np.argmax(y[i])} ({tokenizer.index_word[np.argmax(y[i])]})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"]}]}]}